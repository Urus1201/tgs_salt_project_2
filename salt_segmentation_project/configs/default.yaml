# Default configuration for TGS Salt Segmentation

# Data settings
data:
  base_dir: '${DATA_DIR:/root/tgs_salt_mode_2/data}'  # Default path, can be overridden with env variable
  train_csv: 'train.csv'
  test_csv: 'sample_submission.csv'
  depths_csv: 'depths.csv'
  train_images: 'train/images'
  train_masks: 'train/masks'
  test_images: 'test/images'
  train_dir: 'train'  # Directory containing training images and masks
  val_split: 0.15  # 15% validation split
  use_2_5d: true  # Use 2.5D input (3 consecutive slices)
  img_size: 101
  batch_size: 32
  num_workers: 4
  add_depth_info: true
  prefetch_factor: 2  # Number of batches to prefetch (>1 helps with I/O)
  persistent_workers: true  # Keep workers alive between epochs

# Augmentation settings
augmentation:
  use_augmentation: true
  p_flip: 0.5
  p_rotate: 0.5
  rotate_limit: 45
  brightness_limit: 0.2
  contrast_limit: 0.2
  noise_std: 0.02
  
  # Validation/test augmentations (minimal)
  val_size: 101

# Model settings
model:
  name: "swin_unet"
  swin_variant: "microsoft/swin-base-patch4-window7-224"
  pretrained: true              # Use ImageNet weights
  mae_pretrained: true          # Set to true to use the pretrained weights
  mae_checkpoint: "checkpoints/mae/best_encoder.pth"
  in_channels: 3
  seg_out_channels: 1
  cls_out_channels: 1
  cls_head: true
  # Enhanced model features
  use_deep_supervision: true    # Enable deep supervision
  use_attention_gates: true     # Enable attention gates in decoder
  use_aspp: true                # Enable ASPP module
  use_checkpoint: true          # Enable gradient checkpointing
  dropout_rate: 0.1             # Dropout rate
  use_layer_norm: true          # Use LayerNorm instead of BatchNorm
  cls_use_combined_pooling: true # Use combined avg+max pooling in classifier

# MAE pretraining settings
mae:
  model_name: "microsoft/swin-base-patch4-window7-224"
  pretrained: false  # Start without pretrained weights for pretraining
  img_size: 101
  patch_size: 4
  in_channels: 1  # Align with SwinEncoder default
  embed_dim: 96
  depths: [2, 2, 6, 2]  # Match SwinEncoder structure
  num_heads: [3, 6, 12, 24]  # Match SwinEncoder structure
  window_size: 7
  mlp_ratio: 4.0
  qkv_bias: true
  drop_rate: 0.0  # Regular dropout
  attn_drop_rate: 0.0  # Attention dropout
  drop_path_rate: 0.1  # Stochastic depth
  # Decoder specific settings
  decoder_embed_dim: 48
  decoder_depth: 2
  decoder_num_heads: 4
  mask_ratio: 0.75
  # Training settings
  device: "cuda"
  batch_size: 64
  num_epochs: 15
  learning_rate: 0.0001  # Learning rate for optimizer
  weight_decay: 0.05
  lr_decay_factor: 0.5
  lr_decay_patience: 5
  early_stop_patience: 10
  save_dir: "checkpoints/mae"
  log_dir: "checkpoints/mae/logs"
  vis_dir: "checkpoints/mae/visualizations"

# Training settings
training:
  distributed: false  # Whether to use distributed training
  device: "cuda"  # "cuda" or "cpu"
  num_epochs: 100
  optimizer: "adamw"
  lr: 0.0001
  weight_decay: 0.01
  scheduler: "cosine"  # "cosine" or "plateau"
  warmup_epochs: 5
  early_stop_patience: 15
  grad_clip: 5.0
  save_dir: "checkpoints/logs"
  amp: true  # Use automatic mixed precision
  compile: true  # Use torch.compile() for optimization
  channels_last: true  # Use channels last memory format
  
  # Distributed training settings
  backend: "nccl"  # DDP backend: "nccl" (GPU) or "gloo" (CPU)
  find_unused_parameters: false  # Set to true if needed for DDP
  gradient_as_bucket_view: true  # More efficient DDP gradient sync
  static_graph: true  # Optimize DDP for fixed graphs

# Loss function settings
loss:
  dice_weight: 1.0
  focal_weight: 1.0
  boundary_weight: 0.5
  cls_weight: 0.5
  focal_gamma: 2.0
  focal_alpha: 0.25
  # Deep supervision loss settings
  aux_weight: 0.4    # Weight for auxiliary outputs
  aux_decay: true    # Decay weights with depth

# Inference settings
inference:
  checkpoint: "best_model.pth"
  batch_size: 16
  threshold: 0.5  # Probability threshold for binary prediction
  use_tta: true   # Use test-time augmentation
  use_mc_dropout: true  # Use Monte Carlo dropout for uncertainty
  mc_samples: 30  # Number of MC dropout samples
  min_size: 100
  refinement:
    enabled: true
    uncertainty_threshold: 0.2
    boundary_kernel_size: 3
    crf_iterations: 5

# Refinement model settings
refinement:
  main_model_checkpoint: "${CHECKPOINT_PATH:checkpoints/model_best.pth}"  # Path to main model checkpoint
  checkpoint_path: "checkpoints/refinement_model.pth"  # Where to save refinement model
  epochs: 50
  learning_rate: 1e-4
  weight_decay: 1e-5
  patience: 10
  base_channels: 16
  num_levels: 3
  dropout_rate: 0.2

# Logging settings
logging:
  log_dir: "logs"
  save_images: true
  num_validation_images: 8
  log_every_n_steps: 100
  save_every_n_epochs: 5